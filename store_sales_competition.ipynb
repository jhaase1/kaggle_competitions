{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1c3ac2",
   "metadata": {
    "papermill": {
     "duration": 0.006714,
     "end_time": "2023-12-31T05:24:03.734716",
     "exception": false,
     "start_time": "2023-12-31T05:24:03.728002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea15526",
   "metadata": {
    "papermill": {
     "duration": 0.006548,
     "end_time": "2023-12-31T05:24:03.748278",
     "exception": false,
     "start_time": "2023-12-31T05:24:03.741730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First, import the libraries and set the database path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2d33a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T05:24:03.764013Z",
     "iopub.status.busy": "2023-12-31T05:24:03.763599Z",
     "iopub.status.idle": "2023-12-31T05:24:06.299461Z",
     "shell.execute_reply": "2023-12-31T05:24:06.298233Z"
    },
    "papermill": {
     "duration": 2.547269,
     "end_time": "2023-12-31T05:24:06.302484",
     "exception": false,
     "start_time": "2023-12-31T05:24:03.755215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# library import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86709f29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T05:24:06.319088Z",
     "iopub.status.busy": "2023-12-31T05:24:06.317991Z",
     "iopub.status.idle": "2023-12-31T05:24:06.322969Z",
     "shell.execute_reply": "2023-12-31T05:24:06.321919Z"
    },
    "papermill": {
     "duration": 0.016022,
     "end_time": "2023-12-31T05:24:06.325548",
     "exception": false,
     "start_time": "2023-12-31T05:24:06.309526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oil = pd.read_csv('oil.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "holidays = pd.read_csv('holidays_events.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "train_set = pd.read_csv('train.csv')\n",
    "submission_set = pd.read_csv('test.csv')\n",
    "transactions = pd.read_csv('transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d7282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_mag(x):\n",
    "    k = int(np.argmin(np.abs(x)))\n",
    "    \n",
    "    try:\n",
    "        y = list(x)[k]\n",
    "    except Exception as e:\n",
    "        print(f'x = {x} ({type(x)}), k = {k} ({type(k)})')\n",
    "        raise e\n",
    "        \n",
    "    return y\n",
    "\n",
    "# Boil down holidays to a more manageable set\n",
    "holidays['date'] = pd.to_datetime(holidays['date'])\n",
    "\n",
    "special_days = pd.concat([\n",
    "    pd.merge(\n",
    "        stores,\n",
    "        holidays,\n",
    "        right_on='locale_name',\n",
    "        left_on='city',\n",
    "        suffixes=('_store','_celebration')\n",
    "    ),\n",
    "    pd.merge(\n",
    "        stores,\n",
    "        holidays,\n",
    "        right_on='locale_name',\n",
    "        left_on='state',\n",
    "        suffixes=('_store','_celebration')\n",
    "    ),\n",
    "    pd.merge(\n",
    "        stores,\n",
    "        holidays[holidays.locale == 'National'],\n",
    "        how='cross',\n",
    "        suffixes=('_store','_celebration')\n",
    "    )\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "special_days[['occasion','relative']] = pd.DataFrame(special_days.description.str.split('^(\\D+)\\s*([+-]\\d+)?$').str[1:3].tolist())\n",
    "special_days['relative'] = special_days['relative'].fillna('0').astype(int)\n",
    "\n",
    "special_days['occasion'] = special_days['occasion'].str.replace('^Traslado\\s+(.*)$', '\\\\1', regex=True)\n",
    "\n",
    "special_days['occasion_cat'] = special_days['occasion'].str.lower().str.replace(\n",
    "    '.*(?:fundacion|independencia|provincializacion|batalla).*', 'political', regex=True\n",
    ").str.replace(\n",
    "    '.*(?:cantonizacion).*','cantonizacion', regex=True\n",
    ").str.replace(\n",
    "    '^recupero puente\\s+(.*)$','\\\\1', regex=True\n",
    ").str.replace(\n",
    "    '^puente\\s+(.*)$','\\\\1', regex=True\n",
    ").str.replace(\n",
    "    '.*(?:futbol).*', 'futbol', regex=True\n",
    ").str.replace(\n",
    "    '.*(?:black friday|cyber monday).*', 'shopping', regex=True\n",
    ").str.replace(\n",
    "    '.*(?:terremoto manabi).*', 'earthquake', regex=True\n",
    ").str.replace(\n",
    "    '.*(?:dia del trabajo|dia de la madre|dia de difuntos|viernes santo).*', 'other', regex=True\n",
    ")\n",
    "\n",
    "special_days = special_days.loc[:,[\n",
    "    'store_nbr', 'date', 'type_celebration', 'locale', 'occasion_cat', 'relative'\n",
    "]]\n",
    "\n",
    "special_days = pd.merge(\n",
    "    special_days,\n",
    "    pd.get_dummies(special_days['type_celebration']),\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ").merge(\n",
    "    pd.get_dummies(special_days['occasion_cat']),\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ").merge(\n",
    "    pd.get_dummies(special_days['locale']),\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ").drop(\n",
    "    columns=['type_celebration','occasion_cat','locale']\n",
    ")\n",
    "\n",
    "# handle multiple holidays on the same day\n",
    "special_days = special_days.groupby(\n",
    "    ['store_nbr', 'date']\n",
    ").agg({\n",
    "    'relative': min_mag,\n",
    "    'Additional': 'max',\n",
    "    'Bridge': 'max',\n",
    "    'Event': 'max',\n",
    "    'Holiday': 'max',\n",
    "    'Transfer': 'max',\n",
    "    'Work Day': 'min',\n",
    "    'cantonizacion': 'max',\n",
    "    'carnaval': 'max',\n",
    "    'earthquake': 'max',\n",
    "    'futbol': 'max',\n",
    "    'navidad': 'max',\n",
    "    'other': 'max',\n",
    "    'political': 'max',\n",
    "    'primer dia del ano': 'max',\n",
    "    'shopping': 'max',\n",
    "    'Local': 'max',\n",
    "    'National': 'max',\n",
    "    'Regional': 'max',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ffe48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional info\n",
    "mean_price = 73.694969\n",
    "std_price = 26.581630\n",
    "\n",
    "oil_clean = oil\n",
    "oil_clean['date'] = pd.to_datetime(oil_clean['date'])\n",
    "\n",
    "start_date = oil_clean.date.min()\n",
    "end_date = oil_clean.date.max()\n",
    "desired_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Reindex the DataFrame with the desired date range\n",
    "oil_clean = oil_clean.set_index('date').reindex(desired_date_range).reset_index().rename(columns={'index': 'date'})\n",
    "oil_clean['oil_price'] = oil_clean.pop('dcoilwtico')\n",
    "oil_clean['oil_price'] = oil_clean['oil_price'].fillna(method='ffill')\n",
    "oil_clean['oil_price'] = oil_clean['oil_price'].fillna(method='bfill')# this should just drop Jan 1 of first year\n",
    "oil_clean['oil_price'] = ( oil_clean['oil_price'] - mean_price ) / std_price\n",
    "\n",
    "store_type = pd.get_dummies(stores['type'])\n",
    "store_type = store_type.rename(columns={col: f'store_type_{col}' for col in store_type.columns})\n",
    "\n",
    "store_cluster = pd.get_dummies(stores['cluster'])\n",
    "store_cluster = store_cluster.rename(columns={col: f'store_cluster_{col}' for col in store_cluster.columns})\n",
    "\n",
    "stores_categorization = stores.merge(\n",
    "    store_type,\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ").merge(\n",
    "    store_cluster,\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    ").drop(\n",
    "    columns=['city', 'state', 'type', 'cluster']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d399e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sales (dependent variable), and promotions\n",
    "def variable_extraction(\n",
    "    dataset\n",
    "):\n",
    "    dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "    \n",
    "    predictors = dataset.pivot_table(\n",
    "        index=['date','store_nbr'],\n",
    "        columns='family',\n",
    "        values='onpromotion',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    if 'sales' in dataset:\n",
    "        sales = dataset.pivot_table(\n",
    "            index=['date','store_nbr'],\n",
    "            columns='family',\n",
    "            values='sales',\n",
    "            aggfunc='mean'\n",
    "        ).sort_index()\n",
    "        \n",
    "        #no_store = (sales == 0).all(axis=1).groupby('store_nbr').cummin()\n",
    "        \n",
    "        # remove data for period before store exists\n",
    "        # sales = sales.loc[~no_store, :]\n",
    "        # predictors = predictors.loc[~no_store, :]\n",
    "    else:\n",
    "        sales = None\n",
    "    \n",
    "    predictors = pd.merge(\n",
    "        predictors,\n",
    "        special_days,\n",
    "        on=['date','store_nbr'],\n",
    "        how='left'\n",
    "    ).fillna({\n",
    "        'relative': 0,\n",
    "        'Work Day': True,\n",
    "    }).fillna({key: False for key in [\n",
    "        'Additional','Bridge','Event',\n",
    "        'Holiday','Transfer',\n",
    "        'cantonizacion', 'carnaval', 'earthquake',\n",
    "        'futbol', 'navidad', 'other', 'political',\n",
    "        'primer dia del ano', 'shopping',\n",
    "        'Local', 'National', 'Regional'\n",
    "    ]}\n",
    "    ).reset_index()\n",
    "        \n",
    "    predictors = predictors.merge(\n",
    "        oil_clean,\n",
    "        on='date',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        stores_categorization,\n",
    "        on='store_nbr',\n",
    "        how='left'\n",
    "    ).sort_values([\n",
    "        'store_nbr', 'date'\n",
    "    ]).reset_index(drop=True)\n",
    "        \n",
    "    # seasonality\n",
    "    d = pd.concat([\n",
    "        (predictors.date.dt.day_of_week).rename('day_of_week'),\n",
    "        pd.Series(\n",
    "            np.where(\n",
    "                predictors.date.dt.day <= 15,\n",
    "                15 - predictors.date.dt.day,\n",
    "                predictors.date.dt.days_in_month - predictors.date.dt.day\n",
    "            )\n",
    "        ).rename('semimonthly'),\n",
    "        predictors.date.dt.day.rename('day_of_month'),\n",
    "        predictors.date.map(quarter_part).rename('quarterly'),\n",
    "        predictors.date.dt.month.rename('month'),\n",
    "    ], axis=1)\n",
    "    \n",
    "    #cosD = np.cos(d).rename(columns={col: 'cos_' + col for col in d.columns})\n",
    "    #sinD = np.sin(d).rename(columns={col: 'sin_' + col for col in d.columns})\n",
    "    \n",
    "    predictors = pd.concat([\n",
    "        predictors,\n",
    "        #cosD,\n",
    "        #sinD,\n",
    "        d\n",
    "    ], axis=1)\n",
    "    \n",
    "    store_OHE = pd.get_dummies(predictors.store_nbr)\n",
    "    store_OHE = store_OHE.rename(columns={col: f'store_{col}' for col in store_OHE.columns})\n",
    "        \n",
    "    predictors = pd.merge(\n",
    "        predictors,\n",
    "        store_OHE,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    predictors = predictors.set_index(['date','store_nbr']).sort_index()\n",
    "    \n",
    "    return (predictors, sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6357a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T05:24:06.354376Z",
     "iopub.status.busy": "2023-12-31T05:24:06.353946Z",
     "iopub.status.idle": "2023-12-31T05:24:10.100083Z",
     "shell.execute_reply": "2023-12-31T05:24:10.098841Z"
    },
    "papermill": {
     "duration": 3.757153,
     "end_time": "2023-12-31T05:24:10.102994",
     "exception": false,
     "start_time": "2023-12-31T05:24:06.345841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data import\n",
    "train_df, sales = variable_extraction(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a6d1ee",
   "metadata": {
    "papermill": {
     "duration": 0.007739,
     "end_time": "2023-12-31T05:24:12.592754",
     "exception": false,
     "start_time": "2023-12-31T05:24:12.585015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's create and train our model. In this step, I will use a RandomForestRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53880e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training with RandomForest\n",
    "X = train_df.astype(float) #.drop(['sales', 'date', 'onpromotion', 'store_nbr'], axis=1)\n",
    "y = sales\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c3d8d2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T05:24:12.611066Z",
     "iopub.status.busy": "2023-12-31T05:24:12.610045Z",
     "iopub.status.idle": "2023-12-31T05:27:51.954812Z",
     "shell.execute_reply": "2023-12-31T05:27:51.953512Z"
    },
    "papermill": {
     "duration": 219.364979,
     "end_time": "2023-12-31T05:27:51.965674",
     "exception": false,
     "start_time": "2023-12-31T05:24:12.600695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=40, n_estimators=75, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=40, n_estimators=75, n_jobs=-1, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=40, n_estimators=75, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating and training the RandomForest model\n",
    "model = RandomForestRegressor(n_estimators=50, max_depth=40, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0cb1d",
   "metadata": {
    "papermill": {
     "duration": 0.008639,
     "end_time": "2023-12-31T05:27:51.982725",
     "exception": false,
     "start_time": "2023-12-31T05:27:51.974086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Check the model quality using RMSLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36b4c4d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-31T05:27:52.001755Z",
     "iopub.status.busy": "2023-12-31T05:27:52.000639Z",
     "iopub.status.idle": "2023-12-31T05:27:52.746350Z",
     "shell.execute_reply": "2023-12-31T05:27:52.744815Z"
    },
    "papermill": {
     "duration": 0.758099,
     "end_time": "2023-12-31T05:27:52.749015",
     "exception": false,
     "start_time": "2023-12-31T05:27:51.990916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE: 1.3580584896842083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1200349/2614005188.py:9: RuntimeWarning: invalid value encountered in log1p\n",
      "  log_pred = np.log1p(y_pred)\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# RMSLE\n",
    "log_actual = np.log1p(y_test)\n",
    "log_pred = np.log1p(y_pred)\n",
    "\n",
    "rmsle = np.sqrt(np.mean(np.mean((log_pred - log_actual) ** 2, axis=0)))\n",
    "\n",
    "print(\"RMSLE:\", rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a7bc2",
   "metadata": {
    "papermill": {
     "duration": 0.008293,
     "end_time": "2023-12-31T05:27:52.765892",
     "exception": false,
     "start_time": "2023-12-31T05:27:52.757599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, create the file for the submission using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c6590c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df, _ = variable_extraction(submission_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aee058ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_submit = pd.DataFrame(\n",
    "    model.predict(submission_df.astype(float)),\n",
    "    index=submission_df.index,\n",
    "    columns=sales.columns\n",
    ")\n",
    "\n",
    "submission_results = pd.merge(\n",
    "    submission_set.drop(columns=['onpromotion']),\n",
    "    pd.melt(\n",
    "        y_pred_submit,\n",
    "        ignore_index = False,\n",
    "        value_name='sales'\n",
    "    ).reset_index(),\n",
    "    how='outer'\n",
    ").sort_values('id').loc[\n",
    "    :,\n",
    "    ['id', 'sales']    \n",
    "]\n",
    "\n",
    "submission_results.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 2887556,
     "sourceId": 29781,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 233.54616,
   "end_time": "2023-12-31T05:27:53.783055",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-31T05:24:00.236895",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
